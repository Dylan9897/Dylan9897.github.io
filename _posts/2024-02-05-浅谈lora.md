---
layout:     post   				    # 使用的布局（不需要改）
title:      浅谈lora              # 标题 
subtitle:   大模型微调	                # 副标题
date:       2024-02-05 				# 时间
author:     BY Handx				# 作者
header-img: img/a7d6988001f845fc91101cb04f67c290_0.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 大模型微调
    - lora
---

# 浅谈LoRA



### 简介：

​       LoRA（Low-Rank Adaptation），是微软的研究人员为了解决大语言模型微调而开发的一项技术。深度学习种模型参数的梯度更新策略为：$ W_t=W_{t-1}-\eta\nabla{L} $    由此我们可以延申到对于大模型微调技术的思考： $ W_{new}=W_0+\Delta{W} $   假设 $W_{new}$ 是我们微调后大模型的参数， $W_0$ 是大模型的预训练的参数，常规的训练或微调的方法是基于梯度更新$W_0$的参数，但是在LoRA中，我们关心的是参数更新中的增量 $\Delta{W}$ ，且最终训练的目标也是增量$\Delta{W}$，LoRA的主要工作是对增量$\Delta{W}$做了低秩分解的假设，限制了参数量的更新，并且达到了很好的效果。

### 研究背景

​       NLP任务中通常使用预训练好的模型通过微调来适应下游任务，但是微调会更新模型的所有参数，这样会带量以下的问题：

​       1、对于大模型的参数更新成本非常的大（相当于重新训练）。

​       2、参数更新会导致大模型的灾难性遗忘的问题。

​       对于以上问题，很多研究尝试只通过调整部分参数或为新任务学习外部模块来缓解这一问题，这样只需要在每个预训练模型的基础上，存储和加载少量的特定的任务参数即可。然而现有的技术往往通过扩展模型深度或减少模型可用的序列长度来引入延时推理，例如：

​       （1）基于Adapter-Tuning 增加模型层数，引入了额外的推理延迟。

​       （2）基于Prefix-Tuning的方式难于训练，且预留给Prompt的序列长度占用了下游任务的输入序列空间，影响模型的性能。

​       （3）对于P-Tuning-v2的方式，会导致大模型对原有的问题出现“遗忘”。

​       对于以上存在的问题，LoRA假设模型在学习的过程中权重的变化具有较低的“内在秩”，从而提出低秩适应(LoRA)的方法。

#### ![v2-ac08f1bdb6533756dc93ac7ce86a1a70_720w.png](https://s2.loli.net/2024/02/05/BNFRDV8fmavHwrp.png)

#### **LoRA的数学表示**

​	LoRA的方法与训练的目标无关，但是论文专注于语言模型的建模作为样例。

​	假设有一个预训练的自回归的语言模型$P_\Phi(y|x)$，其中$\Phi$表示模型的参数，在全面微调的期间，模型被初始化为预先训练好的权重$\Phi_0$，并通过梯度更新$\Phi_0+\Delta_\Phi$，使得语言模型的目标最大化： 
$$
\max_{\Phi}\sum_{(x,y)\epsilon{Z}}\sum_{t=1}^{|y|}log(P_\Phi(y_t|x,y_{<t}))
$$
​	而LoRA采用了更加高效的方法，其中特定任务的参数增量$\Delta\Phi=\Delta\Phi(\Theta)$被更小规模的参数集$\Theta$进一步编码，且参数量$|\Theta|<<|\Phi_0|$（约为1：100），因此寻找$\Delta\Phi$的任务变成了对$\Theta$的优化：
$$
\max_{\Phi}\sum_{(x,y)\epsilon{Z}}\sum_{t=1}^{|y|}log(P_{\Phi+\Delta\Phi(\Theta)}(y_t|x,y_{<t}))
$$

#### LoRA的优势：

​		1、一个余弦训练好的模型可以被共享，并用于为不同任务建立许多小的LoRA模块。即可以共享预训练模型，并通过替换图1中的$AB$矩阵来有效的切换任务，从而大大的降低了存储需求和任务切换的难度。

​        2、LoRA的训练更加有效，硬件的门槛降低了3倍，因为训练过程中不需要计算梯度、不需要维护大多数参数的优化器状态，只需要优化注入的、小得多的低秩矩阵。

​        3、简单的线性设计允许在模型部署的时候可以将可训练矩阵与预训练权重合并，与完全微调的模型相比，在结构上没有引入推理延迟。

​        4、LoRA与许多先前的方法是正交的，并可以与许多方法相结合，例如LoRA+P-Tuninig。

#### 低秩矩阵的更新

​	而LoRA假设在权重更新的过程中，通过一个较低的“内在秩”来限制参数的更新： 
$$
 W_0+\Delta{W}=W_0+BA 
$$
​	其中$W_0\epsilon{R^{d*k}}$，$B\epsilon{R^{d*r}}$，$A\epsilon{R^{r*k}}$，其中秩$r<<\min{(d,k)}$，在训练过程中$W_0$被冻结，不接受更新，而$A、B$包含可训练参数。则目标模型的前向传递公式为：
$$
 h=W_0*x+\Delta{W*x}=W_0*x+B*A*x 
$$
​	  实验中对$A$使用高斯分布的随机初始化，对于$B$使用零初始化，所以$\Delta{W}=AB$在训练开始时为零，然后将$\Delta{W_x}$缩放为$\dfrac{\alpha}{r}$，其中$\alpha$是$r$中的一个常数，在进行训练的过程中不断调整，调整$\alpha$与调整学习率的过程大致相同，其中矩阵$A$、矩阵$B$的更新方式如下。

**矩阵**$B$**的更新方式**：
$$
 B_t=B_{t-1}-\eta\frac{\delta{L}}{\delta{B_{t-1}}} 
$$

$$
 \frac{\delta{L}}{\delta{B}}=\frac{\delta{L}}{\delta{W}}A^T 
$$

**矩阵**$A$**的更新方式：**
$$
 A_t=A_{t-1}-\eta\frac{\delta{L}}{\delta{A_{t-1}}} 
$$

$$
 \frac{\delta{L}}{\delta{A}}=B^T\frac{\delta{L}}{\delta{W}} 
$$

**增量的低秩分解证明过程如下：**

 $\Delta{W}=BA $根据矩阵$A$、矩阵$B$的更新方式，则有
$$
 \Delta{W_t}=B_tA_t=(B_{t-1}-\eta{\frac{\delta{L}}{\delta{B_t-1}}})(A_{t-1}-\eta{\frac{\delta{L}}{\delta{A_t-1}}}) 
$$
根据交换律，则有： 
$$
 \Delta{W}=B_{t-1}A_{t-1}-\eta{\frac{\delta{L}}{\delta{B_{t-1}}}}*A_{t-1}-B_{t-1}*\eta{\frac{\delta{L}}{\delta{A_{t-1}}}}+\eta^2{\frac{\delta{L}}{\delta{A_{t-1}}}*\frac{\delta{L}}{\delta{B_{t-1}}}} 
$$
令：
$$
 f_1=\eta{\frac{\delta{L}}{\delta{B_{t-1}}}}*A_{t-1} 
$$

$$
 f_2=B_{t-1}*\eta{\frac{\delta{L}}{\delta{A_{t-1}}}}
$$

$$
 f_3=\eta^2{\frac{\delta{L}}{\delta{A_{t-1}}}*\frac{\delta{L}}{\delta{B_{t-1}}}} 
$$

根据矩阵$A$、矩阵$B$的更新公式： 
$$
f_1=\eta{\frac{\delta{L}}{\delta{B_{t-1}}}}*A_{t-1}=\eta{\frac{\delta{L}}{\delta{W}}A_{t-1}^T}*A_{t-1}=\eta{\frac{\delta{L}}{\delta{W}}} 
$$
同理可得：
$$
f_2=\eta{\frac{\delta{L}}{\delta{W}}} 
$$
对$f_3$进行分析，则有：
$$
 f_3={\eta^2}(\frac{\delta{L}}{\delta{W}}A_{t-1}^TB_{t-1}^T\frac{\delta{L}}{\delta{W}})=\eta^2(\frac{\delta{L}}{\delta{W}})^2B_{t-1}A_{t-1} 
$$
综上所述：
$$
 \Delta{W}=B_{t-1}A_{t-1}-f_1-f_2+f_3 
$$

$$
 \Delta{W}=B_{t-1}A_{t-1}-2\eta{\frac{\delta{L}}{\delta{W}}}+\eta^2(\frac{\delta{L}}{\delta{W}})^2B_{t-1}A_{t-1} 
$$

### LoRA为什么快:

（1）只更新部分参数，例如LoRA原论文中只更新Self-Attention的参数，实际使用时可以选择更新部分层的参数。

（2）减少通信时间，由于更新的参数量少，所以需要传输的数据量也就变少了，尤其是在多卡训练过程中。

（3）采用了混合精度的加速技术。

### 思考：

1、为什么需要缩放，缩放的意义在哪里？

   本人理解为增量$\Delta{W_x}$的缩放因子为$\dfrac{\alpha}{r}$，其中$\alpha$是$r$中的一个常数，在进行训练的过程中不断调整，所以可理解为缩放也是训练的一部分。

2、大模型对于语义理解的程度很强，但是做句子间相似度计算模块效果一般，如何充分调用大模型的语义理解能力。

3、LoRA是一种通用的方法，适用于任何的深度学习中的密集层，那么训练后的大模型的能力是否会出现例如遗忘等问题，例如考虑以下维度“**准确性>指令服从性>信息覆盖率>可读性>无害性**”
