---
layout:     post   				    # 使用的布局（不需要改）
title:      代码记录              # 标题 
subtitle:   代码记录	                # 副标题
date:       2024-04-29 				# 时间
author:     BY Handx				# 作者
header-img: img/a7d6988001f845fc91101cb04f67c290_0.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - FLASK
    - HTML解析
---

# 代码记录

## flask挂载文件

```python
from flask import Flask, render_template

app = Flask(__name__, static_url_path='/target', static_folder='statics')

if __name__ == '__main__':
  app.run(host='0.0.0.0', port=20061)
```

使用说明：

​	将文件打包后放在statics文件夹下，运行程序后即可下载。

下载命令

```bash
wget http://0.0.0.0:20061/target/folder.zip
```



## HTML解析

配置文件

```python
"""
本代码功能是解析html中的文本内容并进行分块
"""

from typing import Dict, List, Union

from langchain.docstore.document import Document
from langchain.document_loaders.base import BaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter,CharacterTextSplitter

class MyBSHTMLLoader(BaseLoader):
    """Load `HTML` files and parse them with `beautiful soup`."""

    def __init__(
        self,
        content: None,
        file_path: Union[str, None] = None,
        open_encoding: Union[str, None] = None,
        bs_kwargs: Union[dict, None] = None,
        get_text_separator: str = "",
    ) -> None:
        """Initialise with path, and optionally, file encoding to use, and any kwargs
        to pass to the BeautifulSoup object.

        Args:
            file_path: The path to the file to load.
            open_encoding: The encoding to use when opening the file.
            bs_kwargs: Any kwargs to pass to the BeautifulSoup object.
            get_text_separator: The separator to use when calling get_text on the soup.
        """
        try:
            import bs4  # noqa:F401
        except ImportError:
            raise ImportError(
                "beautifulsoup4 package not found, please install it with "
                "`pip install beautifulsoup4`"
            )

        if file_path == None and content == None:
            raise Exception(
                "At least one between content and file_path is not empty"
            )
        self.file_path = file_path
        self.content = content
        self.open_encoding = open_encoding
        if bs_kwargs is None:
            bs_kwargs = {"features": "lxml"}
        self.bs_kwargs = bs_kwargs
        self.get_text_separator = get_text_separator

    def load(self) -> List[Document]:
        """Load HTML document into document objects."""
        from bs4 import BeautifulSoup
        if self.content:

            if isinstance(self.content,str):
                # 传入的是html文本格式
                soup = BeautifulSoup(self.content, **self.bs_kwargs)
            else:
                # 传入的是 bs4.dom.tree 格式
                soup = self.content
        else:
            # 传入的是文本文件的路径
            with open(self.file_path, "r", encoding=self.open_encoding) as f:
                soup = BeautifulSoup(f, **self.bs_kwargs)

        text = soup.get_text(self.get_text_separator)

        if soup.title:
            title = str(soup.title.string)
        else:
            title = ""

        metadata: Dict[str, Union[str, None]] = {
            "source": self.file_path,
            "title": title,
        }
        return [Document(page_content=text, metadata=metadata)]

def TextSplitter(content,methods=False,k=3):
    """
    对解析后的html内容进行分块
    :param content:
    :param methods:
    :return:
    """
    # content = content.replace("\n","").replace("\t","")
    if methods:
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            add_start_index=True
        )
    else:
        text_splitter = CharacterTextSplitter(
            separator="\n"*k,
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
    texts = text_splitter.create_documents([content])
    return texts

def get_title_meta(header,interesting_names = ['keywords', 'description']):
    if header.title:
        title_content = header.title.text
    else:
        title_content = ""
    print(f"title content is {title_content}")

    meta_data = {
        "keywords":"",
        "description":""
    }
    # 找到<head>标签下的所有<meta>标签，并筛选出name属性符合要求的标签
    for meta in header.find_all('meta'):
        if meta.get('name') in interesting_names and meta.get('content') is not None:
            # print(f"{meta['name']}: {meta['content']}")
            meta_data[meta['name']] = meta['content']
    return title_content,meta_data

if __name__ == '__main__':
    from bs4 import BeautifulSoup
    with open("test.html","r",encoding="utf-8") as fl:
        html_content = fl.read()
        soup = BeautifulSoup(html_content,'lxml')
        header = soup.head
        title_content,meta_data = get_title_meta((header))
        body = soup.body

        body_obj = MyBSHTMLLoader(content=body)
        body_data = body_obj.load()
        body_content = body_data[0].page_content
        content = TextSplitter(content=body_content)
        for unit in content:
            print(unit)
            s = input()

```

解析文件

```python
import re
from llm_utils import MyBSHTMLLoader
from readability.readability import Document
from newspaper import Article

class Parser():
    def __init__(self):
        pass

    def clean_string(self,text):
        """
        This function takes in a string and performs a series of text cleaning operations.

        Args:
            text (str): The text to be cleaned. This is expected to be a string.
        Returns:
            cleaned_text (str): The cleaned text after all the cleaning operations
            have been performed.
        """
        cleaned_text = re.sub(r"\s+", " ", text.strip())
        cleaned_text = cleaned_text.replace("\\", "")
        cleaned_text = cleaned_text.replace("#", " ")
        cleaned_text = re.sub(r"([^\w\s])\1*", r"\1", cleaned_text)
        return cleaned_text

    def dom_helper(self,content):
        """
        解析html中的文本
        """
        cntnt = ""
        loaders = MyBSHTMLLoader(content)
        loaders = loaders.load()
        for unit in loaders:
            cntnt += unit.page_content + ";"
        return cntnt

    def readability_parser(self,html_content):
        """
        使用 readability 解析html网页文本
        :param html_content:
        :return:
        """
        doc = Document(html_content)
        # 提取清理过的正文内容
        cleaned_text = doc.summary(html_partial=True)  # 或者使用doc.html()获取带有格式的HTML文本
        context = self.dom_helper(cleaned_text)
        return {
            "title":doc.title(),
            "text":context
        }

    def news_parser(self,url,html_content):
        """
        使用 newspaper 解析html网页文本
        :param url:
        :param html_content:
        :return:
        """
        article = Article(url)
        article.download(input_html=html_content)
        article.parse()

        # 提取文章的标题、摘要和正文
        title = article.title
        summary = article.summary
        text = article.text

        # 打印提取的信息
        return {
            "title":title,
            "summary":summary,
            "text":text
        }

if __name__ == '__main__':
    fun = Parser()
    with open("data/test2.txt", "r", encoding="utf-8") as fl:
        html_content = fl.read()
        res = fun.readability_parser(html_content)
        print(res)

```

