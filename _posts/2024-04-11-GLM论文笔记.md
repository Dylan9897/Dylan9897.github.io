---
layout:     post   				    # 使用的布局（不需要改）
title:      GLM-论文阅读笔记              # 标题 
subtitle:   GLM	                # 副标题
date:       2024-04-11 				# 时间
author:     BY Handx				# 作者
header-img: img/a7d6988001f845fc91101cb04f67c290_0.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 大模型
    - ChatGLM
---

# GLM-论文阅读笔记

## 论文背景

​	文章指出现有的自编码（Bert）和自回归模型（GPT）以及Encoder-Decoder模型（T5）没有一个模型架构能够适用下游所有任务**。**

- **自回归模型（AutoRegressive）**：一种学习left-to-right的语言模型，适用于无条件的长文本生成任务以及few-shot-learning。其缺点是适用的是单项的注意力机制，不能在NLU任务中捕获输入文本的上下文信息。

- **自编码模型（AutoEncoding）**：通过去噪的方式来学习双向的上下文信息的编码器。可以为输入文本提供语义信息，适用于NLU任务，但缺点是生成式任务中效果不好。

- **Encoder-Decoder模型**：双向Attention支持的Encoder，单向Attention或Cross-Attention支持的Decoder，适用于有条件的文本生成任务，例如文本摘要和QA任务。例如T5模型，适用于NLU和NLG任务，但是需要更多的参数。

​	以上三种网络结构在处理所有的NLP任务时不够灵活，虽然有些研究适用多任务学习的方式进行处理，但是自编码和自回归模型其本质上还是有区别。面对上述问题，本文创新构建了一种运用Auto-Regressive-Blank-Filling训练方法的GLM（ General Language Model）网络模型架构。

## 论文方法

### **AutoRegressive Blank Infilling**	

​	如下图所示，GLM（通用语言模型）采纳了自回归空白填充的核心机制，其灵感来源于自编码模型的思路，如BERT模型所采用的掩码语言模型（Masked Language Model, MLM）。具体来说，GLM在预处理阶段会借鉴MLM的做法，即从输入文本中随机选取并移除连续的一段文本（称为跨度或Span），然后，它进一步拓展了这一理念，利用自回归方式训练模型，使模型能够按照原始文本的顺序逐个预测并重构这些被移除的文本跨度，从而实现对输入文本内容的有效理解和生成能力。

![微信图片_20240411203149.png](https://s2.loli.net/2024/04/11/DKHiJMycfq1hsTB.png)

GLM通过灵活调整待填充空白区域（即Span）的数量和长度，有效地适应并优化执行三种不同的预训练任务，并且在这些任务上展现出卓越性能。具体来说：

1. 在设定仅遮蔽单个词语的情况下，GLM的空格填充任务实质上复现了经典的掩码语言建模过程，每一步预测对应着恢复被隐藏的一个词汇单元。
2. 当将两段文本——文本1与文本2衔接，并刻意将整个文本2作为一处整体掩蔽，此时的空格填充任务则转变为了条件下的语言生成任务，模型需要基于文本1的内容去推测出完整的文本2。
3. 更进一步，在极端情况下，如果将所有文本内容均予以掩藏，则空格填充任务相当于进行了无条件的语言生成演练，要求模型自动生成连贯的文本序列。

​	最终，在优化GLM的过程中，作者采取了两种互补的预训练策略，两者交替进行以实现更全面的学习：

1. **文档级预测与生成**：选取文档中的随机文本片段进行掩码处理，该片段长度范围设定为文档长度的50%至100%，旨在促进模型对长程依赖关系的理解和基于上下文的整体生成能力。

2. **句子级预测与生成**：从同一文档中进一步随机抽取完整句子作为掩码文本片段，要求每个片段内的掩码词数总计占整个文档长度的15%，这样设计有助于细化模型在独立句子层面的语言理解和生成精度。

​	尽管GLM整合了BERT、GPT和T5的优点，但在进行预训练阶段时，为了更好地契合预训练的目标并强化模型的文本生成能力，作者仍选择了较长的被掩码文本片段。在微调阶段，为了进一步提升性能，作者还将自然语言理解任务地转化为生成任务，例如，将情感分类任务转换为填补空白的生成式任务。

![微信图片_20240411203655.png](https://s2.loli.net/2024/04/11/JtrpcM8KO1TfFmk.png)

### 2D-Position Encoding

​	首先，在对输入文本进行处理时，采用了一种策略，即将原文本中选定的部分内容进行有目的的遮盖，并以“masked”符号替换原内容，由此形成的局部隐匿版本作为第一部分（Part A）的输入素材。与此同时，那些被遮盖的内容并非简单舍弃，而是经过精心设计的重新排列顺序处理，确保在打乱过程中仍能充分保留并考量各遮盖部分间的信息关联性，这一经过重构的掩盖部分序列则作为第二部分（Part B）的预输入数据。

![image.png](https://s2.loli.net/2024/04/11/NeFHvGOgoUTcnfI.png)

​	对于Part A部分，position 1 表示的是绝对起始位置坐标，而position 2则固定为0。而在Part B部分中，position 1代表待掩盖元素的位置标识，而position 2则指代句内相对于某个参照点的相对位置（起始值为1）。通过这样的设计，模型能够习得预测跨度（span）长度的能力，进而有效地应对下游任务中因文本长度不确定性带来的挑战，确保模型能够灵活适应不同长度文本的生成需求。

![image.png](https://s2.loli.net/2024/04/11/MWdSHnItrakzC4x.png)

​	在处理过程中，将拆分后的A、B两部分内容各自配以相应的二维位置编码，以此共同构成模型的输入数据。对于被掩盖的部分，文章采用特殊的“S:start”标记来标识掩盖片段的起始位置；而在输出阶段，模型预测的结果则以“E:end”标签来精确指示每个预测跨度（span）的终止位置，从而完成对原始文本中被遮蔽信息的有效还原与推理。

![1280X1280.PNG](https://s2.loli.net/2024/04/11/reGwAENCDvfO8Ba.png)

### Attention模块

​	
